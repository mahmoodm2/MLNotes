{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mahmoud Mohammadi (mmoahm12@uncc) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Using IRIS dataset from SKLearn **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names']) ['setosa' 'versicolor' 'virginica'] ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "print( iris.keys(), iris['target_names'],iris['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 4, 5, 3, 2]\n",
      "[10, 60, 40, 50, 30, 20]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3,4,5,6,]\n",
    "b=[10,20,30,40,50,60]\n",
    "\n",
    "np.random.seed(100000)\n",
    "#idx = np.random.permutation(len(a))\n",
    "#print(idx)\n",
    "#a,b = a[idx], b[idx]\n",
    "\n",
    "np.random.shuffle(a)\n",
    "print(a)\n",
    "\n",
    "np.random.seed(100000)\n",
    "np.random.shuffle(b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5.5,  2.3,  4. ,  1.3],\n",
       "        [ 6. ,  3.4,  4.5,  1.6],\n",
       "        [ 6. ,  3. ,  4.8,  1.8],\n",
       "        [ 5.9,  3. ,  4.2,  1.5],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 4.4,  2.9,  1.4,  0.2],\n",
       "        [ 5.1,  3.4,  1.5,  0.2],\n",
       "        [ 4.9,  3. ,  1.4,  0.2],\n",
       "        [ 6.4,  2.8,  5.6,  2.1],\n",
       "        [ 5.7,  4.4,  1.5,  0.4],\n",
       "        [ 5. ,  3.2,  1.2,  0.2],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 4.9,  2.4,  3.3,  1. ],\n",
       "        [ 6.1,  2.9,  4.7,  1.4],\n",
       "        [ 5.2,  3.5,  1.5,  0.2],\n",
       "        [ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 5.6,  2.5,  3.9,  1.1],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 5. ,  3.6,  1.4,  0.2],\n",
       "        [ 6. ,  2.2,  4. ,  1. ],\n",
       "        [ 4.3,  3. ,  1.1,  0.1],\n",
       "        [ 6.1,  2.6,  5.6,  1.4],\n",
       "        [ 5. ,  3.4,  1.6,  0.4],\n",
       "        [ 4.4,  3.2,  1.3,  0.2],\n",
       "        [ 5.7,  2.9,  4.2,  1.3],\n",
       "        [ 5.1,  2.5,  3. ,  1.1],\n",
       "        [ 6.9,  3.2,  5.7,  2.3],\n",
       "        [ 7.9,  3.8,  6.4,  2. ],\n",
       "        [ 6.5,  3. ,  5.2,  2. ],\n",
       "        [ 5. ,  3. ,  1.6,  0.2],\n",
       "        [ 4.8,  3.4,  1.9,  0.2],\n",
       "        [ 5.6,  2.7,  4.2,  1.3],\n",
       "        [ 5.5,  2.6,  4.4,  1.2],\n",
       "        [ 4.7,  3.2,  1.3,  0.2],\n",
       "        [ 6.8,  2.8,  4.8,  1.4],\n",
       "        [ 5.8,  2.8,  5.1,  2.4],\n",
       "        [ 6. ,  2.9,  4.5,  1.5],\n",
       "        [ 7.4,  2.8,  6.1,  1.9],\n",
       "        [ 5.3,  3.7,  1.5,  0.2],\n",
       "        [ 7.1,  3. ,  5.9,  2.1],\n",
       "        [ 6.3,  3.3,  4.7,  1.6],\n",
       "        [ 6.7,  3. ,  5.2,  2.3],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 5.5,  4.2,  1.4,  0.2],\n",
       "        [ 6.7,  3. ,  5. ,  1.7],\n",
       "        [ 4.9,  2.5,  4.5,  1.7],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 7.7,  3. ,  6.1,  2.3],\n",
       "        [ 6.2,  2.9,  4.3,  1.3],\n",
       "        [ 5.2,  4.1,  1.5,  0.1],\n",
       "        [ 5.1,  3.8,  1.6,  0.2],\n",
       "        [ 6.3,  2.5,  4.9,  1.5],\n",
       "        [ 4.6,  3.2,  1.4,  0.2],\n",
       "        [ 6.4,  3.1,  5.5,  1.8],\n",
       "        [ 5.6,  2.9,  3.6,  1.3],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 6.3,  3.3,  6. ,  2.5],\n",
       "        [ 5.1,  3.5,  1.4,  0.2],\n",
       "        [ 6.9,  3.1,  5.1,  2.3],\n",
       "        [ 6.3,  2.8,  5.1,  1.5],\n",
       "        [ 5.7,  2.6,  3.5,  1. ],\n",
       "        [ 5.5,  2.5,  4. ,  1.3],\n",
       "        [ 6.9,  3.1,  4.9,  1.5],\n",
       "        [ 5.8,  2.6,  4. ,  1.2],\n",
       "        [ 5. ,  2. ,  3.5,  1. ],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 5.8,  2.7,  4.1,  1. ],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 6.2,  3.4,  5.4,  2.3],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 4.5,  2.3,  1.3,  0.3],\n",
       "        [ 6.7,  3.1,  5.6,  2.4],\n",
       "        [ 6.9,  3.1,  5.4,  2.1],\n",
       "        [ 6.3,  2.9,  5.6,  1.8],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 4.8,  3. ,  1.4,  0.3],\n",
       "        [ 6.4,  2.8,  5.6,  2.2],\n",
       "        [ 5.4,  3.9,  1.3,  0.4],\n",
       "        [ 7.2,  3. ,  5.8,  1.6],\n",
       "        [ 6.4,  2.7,  5.3,  1.9],\n",
       "        [ 6.7,  3.3,  5.7,  2.5],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.9,  3.2,  4.8,  1.8],\n",
       "        [ 5.4,  3. ,  4.5,  1.5],\n",
       "        [ 5.8,  4. ,  1.2,  0.2],\n",
       "        [ 6.1,  3. ,  4.9,  1.8],\n",
       "        [ 6.3,  2.5,  5. ,  1.9],\n",
       "        [ 5.9,  3. ,  5.1,  1.8],\n",
       "        [ 6.7,  3.1,  4.7,  1.5],\n",
       "        [ 7.2,  3.6,  6.1,  2.5],\n",
       "        [ 4.6,  3.6,  1. ,  0.2],\n",
       "        [ 5. ,  2.3,  3.3,  1. ],\n",
       "        [ 6.1,  2.8,  4. ,  1.3],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.7,  2.5,  5. ,  2. ],\n",
       "        [ 5.2,  2.7,  3.9,  1.4],\n",
       "        [ 5.4,  3.4,  1.7,  0.2],\n",
       "        [ 5. ,  3.5,  1.6,  0.6],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 5.7,  2.8,  4.1,  1.3],\n",
       "        [ 6.3,  2.3,  4.4,  1.3],\n",
       "        [ 4.7,  3.2,  1.6,  0.2],\n",
       "        [ 6.4,  3.2,  4.5,  1.5],\n",
       "        [ 7.6,  3. ,  6.6,  2.1],\n",
       "        [ 6.2,  2.2,  4.5,  1.5],\n",
       "        [ 7.3,  2.9,  6.3,  1.8],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 5.1,  3.3,  1.7,  0.5],\n",
       "        [ 5.4,  3.7,  1.5,  0.2],\n",
       "        [ 6.7,  2.5,  5.8,  1.8],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 4.8,  3.4,  1.6,  0.2],\n",
       "        [ 4.8,  3. ,  1.4,  0.1],\n",
       "        [ 7.7,  2.6,  6.9,  2.3],\n",
       "        [ 5.7,  3.8,  1.7,  0.3],\n",
       "        [ 5. ,  3.4,  1.5,  0.2],\n",
       "        [ 5.1,  3.5,  1.4,  0.3],\n",
       "        [ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 6.3,  2.7,  4.9,  1.8],\n",
       "        [ 4.8,  3.1,  1.6,  0.2],\n",
       "        [ 6.6,  2.9,  4.6,  1.3],\n",
       "        [ 5.6,  2.8,  4.9,  2. ],\n",
       "        [ 5.1,  3.8,  1.9,  0.4],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 6.7,  3.1,  4.4,  1.4],\n",
       "        [ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 6. ,  2.7,  5.1,  1.6],\n",
       "        [ 4.6,  3.4,  1.4,  0.3],\n",
       "        [ 6.1,  3. ,  4.6,  1.4],\n",
       "        [ 5.5,  3.5,  1.3,  0.2],\n",
       "        [ 7.2,  3.2,  6. ,  1.8],\n",
       "        [ 6.5,  3. ,  5.5,  1.8],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5.5,  2.4,  3.7,  1. ],\n",
       "        [ 6.8,  3. ,  5.5,  2.1],\n",
       "        [ 4.6,  3.1,  1.5,  0.2],\n",
       "        [ 7.7,  2.8,  6.7,  2. ],\n",
       "        [ 5.6,  3. ,  4.5,  1.5],\n",
       "        [ 7. ,  3.2,  4.7,  1.4],\n",
       "        [ 5.4,  3.4,  1.5,  0.4],\n",
       "        [ 6.2,  2.8,  4.8,  1.8],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 7.7,  3.8,  6.7,  2.2],\n",
       "        [ 6.7,  3.3,  5.7,  2.1],\n",
       "        [ 5.7,  2.8,  4.5,  1.3],\n",
       "        [ 6.4,  2.9,  4.3,  1.3]]),\n",
       " array([1, 1, 2, 1, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 2, 0, 1, 1, 0, 1, 0, 2,\n",
       "        0, 0, 1, 1, 2, 2, 2, 0, 0, 1, 1, 0, 1, 2, 1, 2, 0, 2, 1, 2, 0, 0, 1,\n",
       "        2, 0, 2, 1, 0, 0, 1, 0, 2, 1, 2, 2, 0, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0,\n",
       "        2, 1, 0, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 1, 1, 1, 0, 2, 2, 2, 1, 2, 0,\n",
       "        1, 1, 2, 0, 2, 1, 0, 0, 2, 1, 1, 0, 1, 2, 1, 2, 1, 0, 0, 2, 2, 0, 0,\n",
       "        2, 0, 0, 0, 2, 1, 2, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 1, 0, 2, 2, 0, 1,\n",
       "        2, 0, 2, 1, 1, 0, 2, 0, 2, 2, 1, 1]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= iris['data']\n",
    "y = iris['target']\n",
    "np.random.seed(100000)\n",
    "np.random.shuffle(data)\n",
    "np.random.seed(100000)\n",
    "np.random.shuffle(y)\n",
    "data ,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['data'][:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 0, 1, 2, 1, 0, 0, 2, 1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 2,\n",
       "       0, 0, 2, 2, 0, 1, 2, 2, 2, 1, 2, 0, 0, 2, 0, 1, 2, 1, 0, 1, 1, 2, 2,\n",
       "       2, 0, 0, 2, 0, 1, 2, 0, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1, 0, 0, 1, 2, 0,\n",
       "       2, 1, 1, 1, 2, 2, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2, 2, 0, 1, 1, 2, 1, 2,\n",
       "       2, 1, 2, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2, 1, 2, 1, 1, 1, 1, 2, 2,\n",
       "       0, 2, 1, 1, 0, 0, 0, 2, 2, 2, 0, 2, 1, 2, 1, 0, 0, 1, 2, 2, 1, 2, 0,\n",
       "       0, 2, 0, 0, 2, 1, 0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = iris['target']\n",
    "np.random.seed(100000)\n",
    "np.random.shuffle(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NN_Model: Neural Network Class implementing the NN model wiht model building and training functions**\n",
    "\n",
    "**__init__** : initializing the class parameters such as number of inputs , number of outputs, neurons ...\n",
    "\n",
    "**build_model**: Building a simple NN model consisting:\n",
    "\n",
    "1. Model\n",
    "\n",
    "    1. Input Layer: 4*n inputs layer because the IRIS data ( as training data) has 4 features as:    \n",
    "    \n",
    "       - 'sepal length (cm)',\n",
    "     \n",
    "       - 'sepal width (cm)',\n",
    "    \n",
    "       - 'petal length (cm)',\n",
    "    \n",
    "       - 'petal width (cm)'      \n",
    "    \n",
    "    2. One Hidden Layer (with ReLU activation)\n",
    "    \n",
    "        - using Tensorflow [fully_connected](  https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected) API\n",
    "            - **Weights** : Fully connected weight matrix, which is multiplied by the inputs( in each layer)\n",
    "            - **Activation function**: The default value is a ReLU function            \n",
    "            - **Biases** : Zero tensor            \n",
    "    \n",
    "    3. Output Layer:  without Activation function with dimention of n*3 because the IRIS labels has 3 classes\n",
    "    \n",
    "2. Loss Function\n",
    "   -  Average (mean) of ** sparse_softmax_cross_entropy_with_logits()** on logits\n",
    "    \n",
    "3. Training : \n",
    "    - Opitmizer : **GradientDescentOptimizer()**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Model(object):\n",
    "    def __init__(self, sess, n_inputs, n_outputs, n_hidden, \\\n",
    "                 n_neurons, scope, epoch, batch_size, learning_rate):\n",
    "\n",
    "        self.n_inputs= n_inputs\n",
    "        self.n_classes= n_outputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_neurons = n_neurons\n",
    "        \n",
    "        self.scope= scope\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.learning_rate= learning_rate\n",
    "        \n",
    "        self.sess = sess\n",
    "        \n",
    "        # Partitioning the dataset  \n",
    "        X_data , y_labels, y_onehot = self.load_dataset('iris')\n",
    "        \n",
    "        print(\" Data: %s ,  Target: %s , OneHot: %s \" %(X_data.shape , y_labels.shape ,y_onehot.shape ))\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test \\\n",
    "            = train_test_split( X_data , y_labels, test_size = 0.3)\n",
    "            \n",
    "        \n",
    "        self.build_model()\n",
    "       \n",
    "            #iris['data'] , iris['target'].reshape((-1,1)),test_size = 0.3)\n",
    "\n",
    "    def load_dataset(self, dataset_name):\n",
    "        \n",
    "        # Loading IRIS  datafiles (CSV)\n",
    "        dataset = datasets.load_iris()\n",
    "        \n",
    "        data =dataset['data']\n",
    "        \n",
    "        seed= 8000\n",
    "        tf.random_shuffle(data, seed=seed)\n",
    "        \n",
    "                        \n",
    "        y = dataset['target']               \n",
    "        y = y.reshape(y.shape[0])  \n",
    "        \n",
    "        tf.random_shuffle(y, seed=seed)\n",
    "        \n",
    "        y_onehot = np.zeros( (len(y) , self.n_classes), dtype=np.float)        \n",
    "        for i, _ in enumerate(y):\n",
    "            y_onehot[i, y[i]] = 1.0\n",
    "    \n",
    "        \n",
    "        return data, y , y_onehot\n",
    "       \n",
    "    \n",
    "    def make_layer(self, X, n_neurons , activation_fn=\"relu\"):\n",
    "        \n",
    "        with tf.name_scope(self.scope):\n",
    "            n_inputs =(int)(X.shape[1])\n",
    "\n",
    "            init_w = tf.truncated_normal(shape=(n_inputs, n_neurons))\n",
    "            \n",
    "            W = tf.Variable(init_w , name=\"Weights\")\n",
    "            b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "\n",
    "            y = tf.matmul(X, W) + b\n",
    "\n",
    "            if activation_fn == \"relu\":\n",
    "                return tf.nn.relu(y)\n",
    "            else:\n",
    "                return y\n",
    "  \n",
    "    def build_model(self):\n",
    "        \n",
    "        with tf.name_scope(self.scope) :\n",
    "            \n",
    "            inputs_dim= [self.n_inputs]\n",
    "            \n",
    "            y_dim = []\n",
    "            \n",
    "            #self.batch_size\n",
    "            # Defining Input Data Placeholders            \n",
    "            self.inputs= tf.placeholder(tf.float32, shape=[None] + inputs_dim,  name='inputs')\n",
    "            \n",
    "            self.y = tf.placeholder(tf.int32, shape=[None] + y_dim, name='y')\n",
    "\n",
    "          \n",
    "            \n",
    "            # Input Layer with ReLU Activation function as default            \n",
    "            inputs = fully_connected(self.inputs, self.n_neurons )\n",
    "            \n",
    "            #inputs = self.make_layer(self.X_train, self.n_neurons )\n",
    "            \n",
    "            print(\"inputs %s \" %(inputs.shape ))\n",
    "            \n",
    "            \n",
    "            # Hidden Layer with ReLU Activation function as default \n",
    "            hidden1 = fully_connected(inputs, self.n_neurons )\n",
    "            #hidden1 = self.make_layer(inputs, self.n_neurons )\n",
    "            \n",
    "            print(\"Hidern 1 %s \" %(hidden1.shape))\n",
    "            \n",
    "\n",
    "            # Last Layer of model without applying Activation function: Logits\n",
    "            logits = fully_connected(hidden1, self.n_classes, activation_fn=None)  \n",
    "            #logits = self.make_layer(hidden1, self.n_classes, activation_fn=None)  \n",
    "            print(\"Logits %s , Label %s\" %(logits.shape , self.y.shape))\n",
    "            \n",
    "            \n",
    "            # Defining Loss function based on Entropy\n",
    "            \n",
    "            #xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels= self.y , logits= logits)\n",
    "            \n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits= logits ,labels= self.y )\n",
    "\n",
    "            self.loss = tf.reduce_mean(xentropy, name=\"loss\") # Average of all logits\n",
    "            \n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Evaluation of Logits            \n",
    "            evals = tf.nn.in_top_k(tf.cast(logits , tf.float32), self.y, 1)\n",
    "            \n",
    "            self.accuracy = tf.reduce_mean( tf.cast(evals , tf.float32))\n",
    "\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        print(\"Start Training...\\n\")\n",
    "        \n",
    "        #Optimizer        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        #Initializing the Global Variables       \n",
    "        tf.global_variables_initializer().run(session= self.sess)\n",
    "        \n",
    "        #Calculating Number of batches                    \n",
    "        num_batches = len(self.X_train) // self.batch_size \n",
    "        \n",
    "         \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.epoch):\n",
    "            \n",
    "            #print(\"Epoch Index\"+ epoch)\n",
    "            \n",
    "            for idx in range(num_batches):\n",
    "                \n",
    "                #print(\"Batch Index\"+ idx)\n",
    "                \n",
    "                batch_data = self.X_train[idx *  self.batch_size: (idx+1) *  self.batch_size] \n",
    "                batch_labeles= self.y_train[idx *  self.batch_size: (idx+1) *  self.batch_size] \n",
    "                \n",
    "                print(\"Batch Data %s , Batch Label %s\" %(batch_data.shape , batch_labeles.shape))\n",
    "                \n",
    "                with self.sess.as_default():\n",
    "                    \n",
    "                    self.sess.run([optimizer],\n",
    "                        feed_dict={self.inputs: batch_data, \n",
    "                                   self.y:      batch_labeles  \n",
    "                                  })\n",
    "                \n",
    "                    acc_train = self.accuracy.eval( feed_dict={\n",
    "                    self.inputs: batch_data, \n",
    "                    self.y:      batch_labeles  \n",
    "                     })\n",
    "                \n",
    "                    acc_test  = self.accuracy.eval( feed_dict={\n",
    "                    self.inputs: self.X_test, \n",
    "                    self.y:      self.y_test  \n",
    "                                                          \n",
    "                    })\n",
    "                   \n",
    "                print(\"Dataset:[%s] -> Epoch: [%2d][%4d/%4d] time: %4.4f, Acu Train: %.8f Acu Test: %.8f\"\n",
    "                      % ('iris', epoch, idx, num_batches,\n",
    "                         time.time() - start_time, acc_train, acc_test))\n",
    "                \n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data: (150, 4) ,  Target: (150,) , OneHot: (150, 3) \n",
      "inputs (?, 5) \n",
      "Hidern 1 (?, 5) \n",
      "Logits (?, 3) , Label (?,)\n"
     ]
    }
   ],
   "source": [
    "#def main():\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "        nn_obj = NN_Model(\n",
    "            sess,\n",
    "            scope = 'Test', \n",
    "            epoch = 25, \n",
    "            n_inputs = 4, \n",
    "            n_outputs = 3,\n",
    "            n_hidden = 1, # Changing this paramater has no effect\n",
    "            n_neurons = 5,           \n",
    "            batch_size= 5, \n",
    "            learning_rate = 0.01\n",
    "            )\n",
    "        \n",
    "nn_obj.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** One sample of Execution Results with different values for Neurons, Batch Size and Learning Rate **\n",
    "\n",
    "** Rerunnig the model using the same parameters may generate different results!**\n",
    "\n",
    "| Neurons | Batch Size |  Learnign Rate | Train Accuracy % | Test Accuracy % |\n",
    "|---------|------------|----------------|------------------|-----------------|\n",
    "| 5       | 10         | 0.01           | 63.8             | 66.6            |\n",
    "| 5       | 10         | 0.1            | 91.4             | 91.4            |\n",
    "| 5       | 20         | 0.01           | 69.5             | 69.5            |\n",
    "| 5       | 20         | 0.1            | 71               | 71              |\n",
    "| 10      | 10         | 0.01           | 89.5             | 89.5            |\n",
    "| 10      | 10         | 0.1            | 97.1             | 97.1            |\n",
    "| 10      | 20         | 0.01           | 87.6             | 87.6            |\n",
    "| 10      | 20         | 0.1            | 84.7             | 84.7            |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
